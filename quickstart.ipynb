{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain quickstart\n",
    "\n",
    "This notebook is based on [Langchain Quickstart](https://python.langchain.com/docs/get_started/quickstart) tutorial. For more information, please visit [Langchain Documentation](https://python.langchain.com/docs/get_started/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language models\n",
    "\n",
    "There are two types of language models:\n",
    "\n",
    "- LLMs: this is a language model which takes a string as input and returns a string\n",
    "- ChatModels: this is a language model which takes a list of messages as input and returns a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()  # Instantiate the OpenAI LLM\n",
    "chat_model = ChatOpenAI() # Instantiate the OpenAI chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache': None,\n",
       " 'verbose': False,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'client': openai.api_resources.completion.Completion,\n",
       " 'model_name': 'text-davinci-003',\n",
       " 'temperature': 0.7,\n",
       " 'max_tokens': 256,\n",
       " 'top_p': 1,\n",
       " 'frequency_penalty': 0,\n",
       " 'presence_penalty': 0,\n",
       " 'n': 1,\n",
       " 'best_of': 1,\n",
       " 'model_kwargs': {},\n",
       " 'openai_api_key': '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',\n",
       " 'openai_api_base': '',\n",
       " 'openai_organization': '',\n",
       " 'openai_proxy': '',\n",
       " 'batch_size': 20,\n",
       " 'request_timeout': None,\n",
       " 'logit_bias': {},\n",
       " 'max_retries': 6,\n",
       " 'streaming': False,\n",
       " 'allowed_special': set(),\n",
       " 'disallowed_special': 'all',\n",
       " 'tiktoken_model_name': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache': None,\n",
       " 'verbose': False,\n",
       " 'callbacks': None,\n",
       " 'callback_manager': None,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'client': openai.api_resources.chat_completion.ChatCompletion,\n",
       " 'model_name': 'gpt-3.5-turbo',\n",
       " 'temperature': 0.7,\n",
       " 'model_kwargs': {},\n",
       " 'openai_api_key': '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',\n",
       " 'openai_api_base': '',\n",
       " 'openai_organization': '',\n",
       " 'openai_proxy': '',\n",
       " 'request_timeout': None,\n",
       " 'max_retries': 6,\n",
       " 'streaming': False,\n",
       " 'n': 1,\n",
       " 'max_tokens': None,\n",
       " 'tiktoken_model_name': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input for ChatModels is a list of `ChatMessage`. It requires two components:\n",
    "\n",
    "- `content`: This is the content of the message.\n",
    "- `role`: This is the role of the entity from which the `ChatMessage` is coming from.\n",
    "\n",
    "LangChain provide several objects to distinguish the between different roles:\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant\n",
    "- `SystemMessage`: A ChatMessage coming from the system\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call\n",
    "\n",
    "LangChain provides a standard interface to construct prompts for a given language model:\n",
    "\n",
    "- `predict`: Takes in a string, returns a string\n",
    "- `predict_messages`: Takes in a list of messages, returns a message\n",
    "\n",
    "Let's use the `predict` method to generate a response from a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocrates (469â€“399 BCE) was an ancient Greek philosopher and is considered one of the founders of Western philosophy. He is best known for his Socratic method of questioning, which is still used today in many classrooms. He is also known for his famous quote, \"The unexamined life is not worth living.\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(\"Who is Socrates?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Socrates was a Greek philosopher who is considered one of the founders of Western philosophy. He lived in Athens during the 5th century BCE and is known for his Socratic method of questioning, which sought to stimulate critical thinking and self-reflection. Socrates believed in the pursuit of wisdom and the examination of one\\'s own beliefs and values. He was known for engaging in philosophical conversations with his fellow citizens, often challenging their assumptions and encouraging them to think deeply about their ideas. Socrates was eventually put on trial and sentenced to death for \"corrupting the youth\" and \"impiety.\" Despite his execution, his ideas and teachings have had a profound influence on philosophy and continue to be studied and debated today.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict(\"Who is Socrates?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `predict_messages` method to run over a list of messages and generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"Where did Socrates live?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n\\nBot: Socrates was an ancient Greek philosopher who lived in Athens in the 5th century BCE.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.predict_messages(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Bot: Socrates was an ancient Greek philosopher who lived in Athens in the 5th century BCE.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socrates lived in Athens, Greece.\n"
     ]
    }
   ],
   "source": [
    "response = chat_model.predict_messages(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates\n",
    "\n",
    "Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand. A `PromptTemplate` object helps you with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the idea of Socrates on death?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is the idea of Socrates on {topic}?\"\n",
    ")\n",
    "prompt.format(topic=\"death\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PromptTemplates can also be used to produce a list of messages. Here, what happens most often is a ChatPromptTemplate is a list of ChatMessageTemplates. Each ChatMessageTemplate contains instructions for how to format that ChatMessage - its role, and then also its content. Let's take a look at this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"French\",\n",
    "    text=\"I love programming.\",\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\n\\nSystem: J'adore programmer.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output parsers\n",
    "\n",
    "OutputParsers convert the raw output of an LLM into a format that can be used downstream. There are few main type of OutputParsers, including:\n",
    "\n",
    "- Convert text from LLM -> structured information (e.g. JSON)\n",
    "- Convert a ChatMessage into just a string\n",
    "- Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain\n",
    "\n",
    "We can now combine all these into one chain. This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an (optional) output parser. This is a convenient way to bundle up a modular piece of logic. Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'orange']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, human_message_prompt]\n",
    ")\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=chat_prompt,\n",
    "    output_parser=CommaSeparatedListOutputParser(),\n",
    ")\n",
    "chain.run(\"colors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
